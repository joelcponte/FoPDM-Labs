{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gaussian Mixture Model.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.mixture.base import BaseMixture, _check_shape\n",
    "from sklearn.externals.six.moves import zip\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.extmath import row_norms\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Gaussian mixture shape checkers used by the GaussianMixture class\n",
    "\n",
    "def _check_weights(weights, n_components):\n",
    "    \"\"\"Check the user provided 'weights'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : array-like, shape (n_components,)\n",
    "        The proportions of components of each mixture.\n",
    "\n",
    "    n_components : int\n",
    "        Number of components.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights : array, shape (n_components,)\n",
    "    \"\"\"\n",
    "    weights = check_array(weights, dtype=[np.float64, np.float32],\n",
    "                          ensure_2d=False)\n",
    "    _check_shape(weights, (n_components,), 'weights')\n",
    "\n",
    "    # check range\n",
    "    if (any(np.less(weights, 0.)) or\n",
    "            any(np.greater(weights, 1.))):\n",
    "        raise ValueError(\"The parameter 'weights' should be in the range \"\n",
    "                         \"[0, 1], but got max value %.5f, min value %.5f\"\n",
    "                         % (np.min(weights), np.max(weights)))\n",
    "\n",
    "    # check normalization\n",
    "    if not np.allclose(np.abs(1. - np.sum(weights)), 0.):\n",
    "        raise ValueError(\"The parameter 'weights' should be normalized, \"\n",
    "                         \"but got sum(weights) = %.5f\" % np.sum(weights))\n",
    "    return weights\n",
    "\n",
    "\n",
    "def _check_means(means, n_components, n_features):\n",
    "    \"\"\"Validate the provided 'means'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "        The centers of the current components.\n",
    "\n",
    "    n_components : int\n",
    "        Number of components.\n",
    "\n",
    "    n_features : int\n",
    "        Number of features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    means : array, (n_components, n_features)\n",
    "    \"\"\"\n",
    "    means = check_array(means, dtype=[np.float64, np.float32], ensure_2d=False)\n",
    "    _check_shape(means, (n_components, n_features), 'means')\n",
    "    return means\n",
    "\n",
    "\n",
    "def _check_precision_positivity(precision, covariance_type):\n",
    "    \"\"\"Check a precision vector is positive-definite.\"\"\"\n",
    "    if np.any(np.less_equal(precision, 0.0)):\n",
    "        raise ValueError(\"'%s precision' should be \"\n",
    "                         \"positive\" % covariance_type)\n",
    "\n",
    "\n",
    "def _check_precision_matrix(precision, covariance_type):\n",
    "    \"\"\"Check a precision matrix is symmetric and positive-definite.\"\"\"\n",
    "    if not (np.allclose(precision, precision.T) and\n",
    "            np.all(linalg.eigvalsh(precision) > 0.)):\n",
    "        raise ValueError(\"'%s precision' should be symmetric, \"\n",
    "                         \"positive-definite\" % covariance_type)\n",
    "\n",
    "\n",
    "def _check_precisions_full(precisions, covariance_type):\n",
    "    \"\"\"Check the precision matrices are symmetric and positive-definite.\"\"\"\n",
    "    for prec in precisions:\n",
    "        _check_precision_matrix(prec, covariance_type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# von Mises mixture parameters estimators (used by the M-Step)\n",
    "\n",
    "def _estimate_vonMises_parameters(X, resp):\n",
    "    \"\"\"Estimate the Gaussian distribution parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, 1)\n",
    "        The input data array.\n",
    "\n",
    "    resp : array-like, shape (n_samples, n_components)\n",
    "        The responsibilities for each data sample in X.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nk : array-like, shape (n_components,)\n",
    "        The numbers of data samples in the current components.\n",
    "\n",
    "    means : array-like, shape (n_components, 1)\n",
    "        The centers of the current components.\n",
    "\n",
    "    kappas : array-like\n",
    "        The kappas of the current components.\n",
    "    \"\"\"\n",
    "    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    \n",
    "    means = np.arctan(np.dot(resp.T, np.sin(X)) / np.dot(resp.T, np.cos(X)))\n",
    "    \n",
    "    extended_means = -1 * np.tile(means, resp.shape[0])\n",
    "    \n",
    "    A_kappas = np.sum(np.multiply(resp.T, np.cos(extended_means + X.T)), axis = 1) / nk\n",
    "    \n",
    "    kappas = (2 * A_kappas - A_kappas**3) / (1 - A_kappas**2)\n",
    "\n",
    "    return nk, means, kappas[:, np.newaxis]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Gaussian mixture probability estimators\n",
    "def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):\n",
    "    \"\"\"Compute the log-det of the cholesky decomposition of matrices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_chol : array-like,\n",
    "        Cholesky decompositions of the matrices.\n",
    "        'full' : shape of (n_components, n_features, n_features)\n",
    "        'tied' : shape of (n_features, n_features)\n",
    "        'diag' : shape of (n_components, n_features)\n",
    "        'spherical' : shape of (n_components,)\n",
    "\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "\n",
    "    n_features : int\n",
    "        Number of features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    log_det_precision_chol : array-like, shape (n_components,)\n",
    "        The determinant of the precision matrix for each component.\n",
    "    \"\"\"\n",
    "    if covariance_type == 'full':\n",
    "        n_components, _, _ = matrix_chol.shape\n",
    "        log_det_chol = (np.sum(np.log(\n",
    "            matrix_chol.reshape(\n",
    "                n_components, -1)[:, ::n_features + 1]), 1))\n",
    "\n",
    "    elif covariance_type == 'tied':\n",
    "        log_det_chol = (np.sum(np.log(np.diag(matrix_chol))))\n",
    "\n",
    "    elif covariance_type == 'diag':\n",
    "        log_det_chol = (np.sum(np.log(matrix_chol), axis=1))\n",
    "\n",
    "    else:\n",
    "        log_det_chol = n_features * (np.log(matrix_chol))\n",
    "\n",
    "    return log_det_chol\n",
    "\n",
    "\n",
    "def _estimate_log_von_Mises_prob(X, means, kappas):\n",
    "    \"\"\"Estimate the log von Mises probability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, 1)\n",
    "\n",
    "    means : array-like, shape (n_components, 1)\n",
    "\n",
    "    kappas : array-like, shape (n_components, 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    log_prob : array, shape (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components, _ = means.shape\n",
    "    extended_means = -1 * np.tile(means, n_samples).T    #(n_samples, n_components)\n",
    "    log_kappas = np.log( 2* np.pi * np.i0(kappas))       #(n_components, 1)\n",
    "    extended_log_kappas = np.tile(log_kappas[:, np.newaxis], n_samples).T  #(n_samples, n_components)\n",
    " \n",
    "    extended_kappas = np.tile(kappas, n_samples).T       #(n_samples, n_components)\n",
    "    return np.multiply(extended_kappas, np.cos(extended_means + X)) - extended_log_kappas\n",
    "\n",
    "\n",
    "class vonMisesMixture(BaseMixture):\n",
    "    \"\"\"von Mises Mixture.\n",
    "\n",
    "    Representation of a Gaussian mixture model probability distribution.\n",
    "    This class allows to estimate the parameters of a Gaussian mixture\n",
    "    distribution.\n",
    "\n",
    "    Read more in the :ref:`User Guide <gmm>`.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int, defaults to 1.\n",
    "        The number of mixture components.\n",
    "\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'},\n",
    "            defaults to 'full'.\n",
    "        String describing the type of covariance parameters to use.\n",
    "        Must be one of::\n",
    "\n",
    "            'full' (each component has its own general covariance matrix),\n",
    "            'tied' (all components share the same general covariance matrix),\n",
    "            'diag' (each component has its own diagonal covariance matrix),\n",
    "            'spherical' (each component has its own single variance).\n",
    "\n",
    "    tol : float, defaults to 1e-3.\n",
    "        The convergence threshold. EM iterations will stop when the\n",
    "        lower bound average gain is below this threshold.\n",
    "\n",
    "    reg_covar : float, defaults to 1e-6.\n",
    "        Non-negative regularization added to the diagonal of covariance.\n",
    "        Allows to assure that the covariance matrices are all positive.\n",
    "\n",
    "    max_iter : int, defaults to 100.\n",
    "        The number of EM iterations to perform.\n",
    "\n",
    "    n_init : int, defaults to 1.\n",
    "        The number of initializations to perform. The best results are kept.\n",
    "\n",
    "    init_params : {'kmeans', 'random'}, defaults to 'kmeans'.\n",
    "        The method used to initialize the weights, the means and the\n",
    "        precisions.\n",
    "        Must be one of::\n",
    "\n",
    "            'kmeans' : responsibilities are initialized using kmeans.\n",
    "            'random' : responsibilities are initialized randomly.\n",
    "\n",
    "    weights_init : array-like, shape (n_components, ), optional\n",
    "        The user-provided initial weights, defaults to None.\n",
    "        If it None, weights are initialized using the `init_params` method.\n",
    "\n",
    "    means_init : array-like, shape (n_components, n_features), optional\n",
    "        The user-provided initial means, defaults to None,\n",
    "        If it None, means are initialized using the `init_params` method.\n",
    "\n",
    "    precisions_init : array-like, optional.\n",
    "        The user-provided initial precisions (inverse of the covariance\n",
    "        matrices), defaults to None.\n",
    "        If it None, precisions are initialized using the 'init_params' method.\n",
    "        The shape depends on 'covariance_type'::\n",
    "\n",
    "            (n_components,)                        if 'spherical',\n",
    "            (n_features, n_features)               if 'tied',\n",
    "            (n_components, n_features)             if 'diag',\n",
    "            (n_components, n_features, n_features) if 'full'\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    warm_start : bool, default to False.\n",
    "        If 'warm_start' is True, the solution of the last fitting is used as\n",
    "        initialization for the next call of fit(). This can speed up\n",
    "        convergence when fit is called several time on similar problems.\n",
    "\n",
    "    verbose : int, default to 0.\n",
    "        Enable verbose output. If 1 then it prints the current\n",
    "        initialization and each iteration step. If greater than 1 then\n",
    "        it prints also the log probability and the time needed\n",
    "        for each step.\n",
    "\n",
    "    verbose_interval : int, default to 10.\n",
    "        Number of iteration done before the next print.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    weights_ : array-like, shape (n_components,)\n",
    "        The weights of each mixture components.\n",
    "\n",
    "    means_ : array-like, shape (n_components, n_features)\n",
    "        The mean of each mixture component.\n",
    "\n",
    "    covariances_ : array-like\n",
    "        The covariance of each mixture component.\n",
    "        The shape depends on `covariance_type`::\n",
    "\n",
    "            (n_components,)                        if 'spherical',\n",
    "            (n_features, n_features)               if 'tied',\n",
    "            (n_components, n_features)             if 'diag',\n",
    "            (n_components, n_features, n_features) if 'full'\n",
    "\n",
    "    precisions_ : array-like\n",
    "        The precision matrices for each component in the mixture. A precision\n",
    "        matrix is the inverse of a covariance matrix. A covariance matrix is\n",
    "        symmetric positive definite so the mixture of Gaussian can be\n",
    "        equivalently parameterized by the precision matrices. Storing the\n",
    "        precision matrices instead of the covariance matrices makes it more\n",
    "        efficient to compute the log-likelihood of new samples at test time.\n",
    "        The shape depends on `covariance_type`::\n",
    "\n",
    "            (n_components,)                        if 'spherical',\n",
    "            (n_features, n_features)               if 'tied',\n",
    "            (n_components, n_features)             if 'diag',\n",
    "            (n_components, n_features, n_features) if 'full'\n",
    "\n",
    "    precisions_cholesky_ : array-like\n",
    "        The cholesky decomposition of the precision matrices of each mixture\n",
    "        component. A precision matrix is the inverse of a covariance matrix.\n",
    "        A covariance matrix is symmetric positive definite so the mixture of\n",
    "        Gaussian can be equivalently parameterized by the precision matrices.\n",
    "        Storing the precision matrices instead of the covariance matrices makes\n",
    "        it more efficient to compute the log-likelihood of new samples at test\n",
    "        time. The shape depends on `covariance_type`::\n",
    "\n",
    "            (n_components,)                        if 'spherical',\n",
    "            (n_features, n_features)               if 'tied',\n",
    "            (n_components, n_features)             if 'diag',\n",
    "            (n_components, n_features, n_features) if 'full'\n",
    "\n",
    "    converged_ : bool\n",
    "        True when convergence was reached in fit(), False otherwise.\n",
    "\n",
    "    n_iter_ : int\n",
    "        Number of step used by the best fit of EM to reach the convergence.\n",
    "\n",
    "    lower_bound_ : float\n",
    "        Log-likelihood of the best fit of EM.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    BayesianGaussianMixture : Gaussian mixture model fit with a variational\n",
    "        inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,\n",
    "                 reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',\n",
    "                 weights_init=None, means_init=None, precisions_init=None,\n",
    "                 random_state=None, warm_start=False,\n",
    "                 verbose=0, verbose_interval=10):\n",
    "        super(vonMisesMixture, self).__init__(\n",
    "            n_components=n_components, tol=tol, reg_covar=reg_covar,\n",
    "            max_iter=max_iter, n_init=n_init, init_params=init_params,\n",
    "            random_state=random_state, warm_start=warm_start,\n",
    "            verbose=verbose, verbose_interval=verbose_interval)\n",
    "\n",
    "        self.covariance_type = covariance_type\n",
    "        self.weights_init = weights_init\n",
    "        self.means_init = means_init\n",
    "        self.precisions_init = precisions_init\n",
    "\n",
    "    def _check_parameters(self, X):\n",
    "        pass\n",
    "\n",
    "    def _initialize(self, X, resp):\n",
    "        \"\"\"Initialization of the Gaussian mixture parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "\n",
    "        resp : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        weights, means, kappas = _estimate_vonMises_parameters(\n",
    "            X, resp)\n",
    "        weights /= n_samples\n",
    "\n",
    "        self.weights_ = (weights if self.weights_init is None\n",
    "                         else self.weights_init)\n",
    "        self.means_ = means if self.means_init is None else self.means_init\n",
    "\n",
    "        if self.precisions_init is None:\n",
    "            self.kappas_ = kappas\n",
    "    \n",
    "    def _m_step(self, X, log_resp):\n",
    "        \"\"\"M step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "\n",
    "        log_resp : array-like, shape (n_samples, n_components)\n",
    "            Logarithm of the posterior probabilities (or responsibilities) of\n",
    "            the point of each sample in X.\n",
    "        \"\"\"\n",
    "        n_samples, _ = X.shape\n",
    "        self.weights_, self.means_, self.kappas_ = (\n",
    "            _estimate_vonMises_parameters(X, np.exp(log_resp)))\n",
    "        self.weights_ /= n_samples\n",
    "        \n",
    "\n",
    "    def _estimate_log_prob(self, X):\n",
    "        return _estimate_log_von_Mises_prob(\n",
    "            X, self.means_, self.kappas_)\n",
    "\n",
    "    def _estimate_log_weights(self):\n",
    "        return np.log(self.weights_)\n",
    "\n",
    "    def _compute_lower_bound(self, _, log_prob_norm):\n",
    "        return log_prob_norm\n",
    "\n",
    "    def _check_is_fitted(self):\n",
    "        pass\n",
    "    \n",
    "    def _get_parameters(self):\n",
    "        return (self.weights_, self.means_, self.kappas_)\n",
    "\n",
    "    def _set_parameters(self, params):\n",
    "        (self.weights_, self.means_, self.kappas) = params\n",
    "\n",
    "    def _n_parameters(self):\n",
    "        \"\"\"Return the number of free parameters in the model.\"\"\"\n",
    "        _, n_features = self.means_.shape\n",
    "        if self.covariance_type == 'full':\n",
    "            cov_params = self.n_components * n_features * (n_features + 1) / 2.\n",
    "        elif self.covariance_type == 'diag':\n",
    "            cov_params = self.n_components * n_features\n",
    "        elif self.covariance_type == 'tied':\n",
    "            cov_params = n_features * (n_features + 1) / 2.\n",
    "        elif self.covariance_type == 'spherical':\n",
    "            cov_params = self.n_components\n",
    "        mean_params = n_features * self.n_components\n",
    "        return int(cov_params + mean_params + self.n_components - 1)\n",
    "\n",
    "    def bic(self, X):\n",
    "        pass\n",
    "\n",
    "    def aic(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "#from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X_train = np.loadtxt(\"Amerge.txt\")\n",
    "X = X_train[:, 0]\n",
    "X = X[:, np.newaxis]\n",
    "\n",
    "gmm = vonMisesMixture(n_components=2)\n",
    "gmm.fit(X)\n",
    "Y_predict = gmm.predict(X)\n",
    "\n",
    "cluster_0 = X_train[Y_predict == 0]\n",
    "cluster_1 = X_train[Y_predict == 1]\n",
    "\n",
    "print(cluster_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
